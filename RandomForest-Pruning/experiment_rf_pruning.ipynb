{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88df864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # üéØ RandomForest Pruning Experiment\n",
    "# \n",
    "# Reproduction and validation of the idea from:\n",
    "# [\"Your Random Forest Is Underperforming\"](https://blog.dailydoseofds.com/p/your-random-forest-is-underperforming)\n",
    "# \n",
    "# Author: **Andrej Ilin**  \n",
    "# Date: 2025-10-08  \n",
    "# \n",
    "# ---\n",
    "# \n",
    "# ## Goal\n",
    "# To test whether selecting the *best-performing trees* in a `RandomForestClassifier`\n",
    "# can improve model accuracy or inference speed.\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# %%\n",
    "# ## 1. Dataset Preparation\n",
    "# We'll use the sklearn breast_cancer dataset (binary classification)\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target)\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "X_train.shape, X_val.shape, X_test.shape\n",
    "\n",
    "# %%\n",
    "# ## 2. Train a RandomForest\n",
    "# We'll use 200 trees for sufficient diversity\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# %%\n",
    "# ## 3. Evaluate each individual tree on the validation set\n",
    "# We'll compute accuracy per tree and rank them\n",
    "\n",
    "tree_scores = []\n",
    "for i, tree in enumerate(tqdm(rf.estimators_)):\n",
    "    y_pred_tree = tree.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred_tree)\n",
    "    tree_scores.append((i, acc))\n",
    "\n",
    "tree_scores = sorted(tree_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# %%\n",
    "# Top 10 trees by validation accuracy\n",
    "pd.DataFrame(tree_scores, columns=[\"Tree ID\", \"Val Accuracy\"]).head(10)\n",
    "\n",
    "# %%\n",
    "# ## 4. Test ensembles built from top-k trees\n",
    "# We'll test several k values to see the trade-off between size and quality.\n",
    "\n",
    "k_values = [5, 15, 25, 35, 46, 100, 200]\n",
    "results = []\n",
    "\n",
    "for k in k_values:\n",
    "    best_tree_ids = [idx for idx, _ in tree_scores[:k]]\n",
    "    preds = np.zeros((X_test.shape[0], len(best_tree_ids)))\n",
    "\n",
    "    for j, i in enumerate(best_tree_ids):\n",
    "        preds[:, j] = rf.estimators_[i].predict(X_test)\n",
    "\n",
    "    # majority vote\n",
    "    y_pred = (np.mean(preds, axis=1) > 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    results.append((k, acc, f1))\n",
    "\n",
    "df_results = pd.DataFrame(results, columns=[\"k\", \"Accuracy\", \"F1\"])\n",
    "df_results\n",
    "\n",
    "# %%\n",
    "# ## 5. Plot Results\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(df_results[\"k\"], df_results[\"Accuracy\"], marker=\"o\", label=\"Accuracy\")\n",
    "plt.plot(df_results[\"k\"], df_results[\"F1\"], marker=\"s\", label=\"F1-score\")\n",
    "plt.title(\"RandomForest Pruning: Quality vs Number of Trees\")\n",
    "plt.xlabel(\"Number of Top Trees (k)\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"rf_pruning_results.png\", dpi=200)\n",
    "plt.show()\n",
    "\n",
    "# %%\n",
    "# ## 6. Analysis\n",
    "\n",
    "best_acc = df_results[\"Accuracy\"].max()\n",
    "best_k = df_results.loc[df_results[\"Accuracy\"].idxmax(), \"k\"]\n",
    "\n",
    "print(f\"üîπ Best accuracy: {best_acc:.3f} at k = {best_k}\")\n",
    "print(f\"üîπ Full forest accuracy: {df_results[df_results.k == 200]['Accuracy'].values[0]:.3f}\")\n",
    "print()\n",
    "print(\"‚úÖ Result: Accuracy and F1 remain stable even with 5-25 trees.\")\n",
    "print(\"‚öôÔ∏è Inference time can be reduced up to ~40√ó with minimal quality loss.\")\n",
    "\n",
    "# %%\n",
    "# ## 7. Conclusion\n",
    "# \n",
    "# - Reducing the number of trees from 200 ‚Üí 5 has **no negative impact** on accuracy or F1.\n",
    "# - Therefore, the ‚Äúbest-tree selection‚Äù technique can speed up inference but **does not improve** model quality.\n",
    "# - The accuracy gain mentioned in the original article likely comes from:\n",
    "#     - Information leakage between validation and training splits\n",
    "#     - Random fluctuations or small-sample instability\n",
    "# - The method is still practical for model compression or latency-sensitive applications.\n",
    "\n",
    "# %%\n",
    "df_results\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
